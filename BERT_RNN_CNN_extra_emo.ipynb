{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"H_qTHxDX4cop"},"outputs":[],"source":["#pip install --upgrade torch==1.11.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrKPWcM442QP"},"outputs":[],"source":["import pandas as pd\n","from typing import Dict, List\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import torch\n","\n","from sklearn.metrics import f1_score\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","import torch.nn.functional as F\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import scipy\n","import re\n","import math\n","from sklearn.metrics import classification_report\n","from transformers import AutoTokenizer, AutoModel\n","from torch.optim import Adam\n","from sklearn.metrics import classification_report\n","\n","\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kngEkS_44c2"},"outputs":[],"source":["model_name = 'xlm-roberta-base'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-xr2nU747qc"},"outputs":[],"source":["dataset = pd.read_csv('tiny_dataset.csv', encoding = 'utf-8')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcI8CyQu2lKD"},"outputs":[],"source":["dataset.emo_class.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4Yso0BC2lKG"},"outputs":[],"source":["dataset = dataset[(dataset.post_date >= 2019) & (dataset.post_date <= 2020)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DJ4u4wgf2lKR"},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, df):\n","\n","        self.labels = [label for label in df['emo_class']]\n","        # self.labels = [labels[label] for label in df['round_N']]\n","        self.texts = [tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for text in df['text']]\n","\n","    def classes(self):\n","        return self.labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def get_batch_labels(self, idx):\n","        return np.array(self.labels[idx])\n","\n","    def get_batch_texts(self, idx):\n","        return self.texts[idx]\n","\n","    def __getitem__(self, idx):\n","\n","        batch_texts = self.get_batch_texts(idx)\n","        batch_y = self.get_batch_labels(idx)\n","\n","        return batch_texts, batch_y\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ivyLpmZ2lKU"},"outputs":[],"source":["class BertBaseline(nn.Module):\n","    def __init__(self, model_name, inner_features, out_features):\n","        super(BertBaseline, self).__init__()\n","\n","        self.bert = AutoModel.from_pretrained(model_name, return_dict=True)\n","        self.conv = nn.Conv1d(768, 512, kernel_size=1)  # Convolutional layer for dimension reduction\n","        self.rnn = nn.GRU(512, inner_features, batch_first=True)\n","        self.linear1 = nn.Linear(inner_features, inner_features // 2)\n","        self.linear2 = nn.Linear(inner_features // 2, out_features)\n","        self.dropout = nn.Dropout(0.15)\n","        self.layer_norm = nn.LayerNorm(inner_features)\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        bert = self.bert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n","        hidden_states = bert.hidden_states\n","        x = hidden_states[-1]  # Use the last hidden layer\n","        x = x.transpose(1, 2)  # Transpose to make the dimension 768 the last dimension\n","        x = self.conv(x)  # Apply the convolutional layer for dimension reduction\n","        x = x.transpose(1, 2)  # Transpose back to the original dimension\n","        x = self.rnn(x)[0]  # Apply the RNN and get the output sequence\n","        x = self.layer_norm(x)  # Apply layer normalization\n","        x = self.dropout(x)  # Apply dropout\n","        x = self.relu(x)  # Apply ReLU activation\n","        x = x[:, -1, :]  # Take the last output from the recurrent layer\n","        x = self.linear1(x)  # Apply the first linear layer\n","        x = self.relu(x)  # Apply ReLU activation\n","        x = self.dropout(x)  # Apply dropout\n","        x = self.linear2(x)  # Apply the second linear layer\n","        x = self.softmax(x)  # Apply softmax activation\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jBwwiEo2lKZ"},"outputs":[],"source":["def train(model, train_data, val_data, learning_rate, epochs):\n","    val_reports = []\n","    train_reports = []\n","    best_epoch = 0\n","    train, val = Dataset(train_data), Dataset(val_data)\n","    train_dataloader = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True)\n","    val_dataloader = torch.utils.data.DataLoader(val, batch_size=16)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = Adam(model.parameters(), lr=learning_rate)\n","\n","    if use_cuda:\n","        model = model.cuda()\n","        criterion = criterion.cuda()\n","\n","    min_val_loss = float('inf')\n","    for epoch_num in range(epochs):\n","        total_acc_train = 0\n","        total_loss_train = 0\n","\n","        train_dataloader_desc = f\"Training (Epoch {epoch_num+1}/{epochs})\"\n","        train_dataloader_tqdm = tqdm(train_dataloader, desc=train_dataloader_desc, leave=False)\n","\n","        for train_input, train_label in train_dataloader_tqdm:\n","            train_label = train_label.to(device)\n","            mask = train_input['attention_mask'].to(device)\n","            input_id = train_input['input_ids'].squeeze(1).to(device)\n","\n","            output = model(input_id, attention_mask=mask)\n","\n","            batch_loss = criterion(output, train_label.long())\n","            total_loss_train += batch_loss.item()\n","\n","            acc = (output.argmax(dim=1) == train_label).sum().item()\n","            total_acc_train += acc\n","\n","            model.zero_grad()\n","            batch_loss.backward()\n","            optimizer.step()\n","\n","            train_dataloader_tqdm.set_postfix({'Loss': total_loss_train / (train_dataloader_tqdm.n + 1e-12)})\n","\n","        total_acc_val = 0\n","        total_loss_val = 0\n","\n","        with torch.no_grad():\n","            for val_input, val_label in val_dataloader:\n","                val_label = val_label.to(device)\n","                mask = val_input['attention_mask'].to(device)\n","                input_id = val_input['input_ids'].squeeze(1).to(device)\n","\n","                output = model(input_id, mask)\n","\n","                batch_loss = criterion(output, val_label.long())\n","                total_loss_val += batch_loss.item()\n","\n","                acc = (output.argmax(dim=1) == val_label).sum().item()\n","                total_acc_val += acc\n","        avg_train_loss = total_loss_train /len(train_dataloader_tqdm)\n","        avg_val_loss = total_loss_val /len(val_dataloader)\n","\n","\n","        if avg_val_loss <= min_val_loss:\n","            min_val_loss = avg_val_loss\n","            best_epoch = epoch_num +1\n","            print('best epoch', best_epoch)\n","            path = 'tiny_best_rnn_cnn__extra_emo'+str(best_epoch) +'.pth'\n","            torch.save(model.state_dict(), path)\n","\n","        train_labels = []\n","        train_predictions = []\n","        for train_input, train_label in train_dataloader:\n","            train_label = train_label.to(device)\n","            mask = train_input['attention_mask'].to(device)\n","            input_id = train_input['input_ids'].squeeze(1).to(device)\n","\n","            output = model(input_id, attention_mask=mask)\n","\n","            _, predict = torch.max(output.cpu().data, 1)\n","            train_labels.extend(train_label.cpu().detach().numpy())\n","            train_predictions.extend(predict.cpu().detach().numpy())\n","\n","        train_report = classification_report(train_labels, train_predictions)\n","        train_reports.append(train_report)\n","        val_labels = []\n","        val_predictions = []\n","        for val_input, val_label in val_dataloader:\n","            val_label = val_label.to(device)\n","            mask = val_input['attention_mask'].to(device)\n","            input_id = val_input['input_ids'].squeeze(1).to(device)\n","\n","            output = model(input_id, attention_mask=mask)\n","\n","            _, predict = torch.max(output.cpu().data, 1)\n","            val_labels.extend(val_label.cpu().detach().numpy())\n","            val_predictions.extend(predict.cpu().detach().numpy())\n","\n","        validation_report = classification_report(val_labels, val_predictions)\n","        val_reports.append(validation_report)\n","\n","        print(f'Epoch: {epoch_num + 1} | Train Loss: {avg_train_loss:.3f} | Val Loss: {avg_val_loss:.3f}')\n","        print('train report')\n","        print(train_report)\n","        print('validation report')\n","        print(validation_report)\n","\n","    return path,best_epoch,train_reports, val_reports\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPtHJUrA2lKd"},"outputs":[],"source":["def evaluate(model, test_data):\n","    test = Dataset(test_data)\n","    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n","\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","\n","    if use_cuda:\n","\n","\n","        model = model.cuda()\n","\n","\n","    total_acc_test = 0\n","    test_labels = []\n","    test_predictions = []\n","    with torch.no_grad():\n","\n","\n","        for test_input, test_label in test_dataloader:\n","\n","\n","            test_label = test_label.to(device)\n","            mask = test_input['attention_mask'].to(device)\n","            input_id = test_input['input_ids'].squeeze(1).to(device)\n","\n","\n","            output = model(input_id, mask)\n","\n","\n","            acc = (output.argmax(dim=1) == test_label).sum().item()\n","            total_acc_test += acc\n","            _, predict = torch.max(output.cpu().data, 1)\n","            test_labels.extend(test_label.cpu().detach().numpy())\n","            test_predictions.extend(predict.cpu().detach().numpy())\n","\n","        test_report = classification_report(test_labels, test_predictions)\n","        print(test_report)\n","    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptJrePwv2lKe"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F2ne-yEO2lKf"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","df_train, df_test = train_test_split(dataset, test_size=0.3, stratify=dataset['emo_class'],\n","                                   random_state=42)\n","df_val, df_test = train_test_split(df_test, test_size=0.5, stratify=df_test['emo_class'],\n","                                   random_state=42)\n","low_class = df_train[df_train.emo_class == 2]\n","df_train = df_train.append(low_class)\n","df_train = df_train.reset_index(drop=True)\n","df_val = df_val.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FkNHAyZ2lKh"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z0k2GPVm2lKh"},"outputs":[],"source":["EPOCHS = 10\n","model = BertBaseline(model_name, inner_features=512, out_features=3)\n","LR = 1e-6\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"CnyHQ8Xg2lKj"},"outputs":[],"source":["path,best_epoch,train_reports, val_reports = train(model, df_train, df_val, LR, EPOCHS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i04FrE0t2lKl"},"outputs":[],"source":["print(train_reports[best_epoch-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VqU3efrQ2lKm"},"outputs":[],"source":["print(val_reports[best_epoch-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFy2rzy72lKn"},"outputs":[],"source":["model = BertBaseline('xlm-roberta-base', inner_features = 512, out_features = 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sid0J8cQ2lKo"},"outputs":[],"source":["model.load_state_dict(torch.load(path))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-6VtFkV2lKo"},"outputs":[],"source":["model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kcgjOzO2lKp"},"outputs":[],"source":["evaluate(model, df_test)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1qfQlzi0WKSosw6U6OWmNwZLZ5EGp_XTv","timestamp":1686681635144},{"file_id":"1RbvW36RyLdr6qpiXtiY0_Ekk0SJYXFJK","timestamp":1686679954195},{"file_id":"1gCyXDuLYTNoDqN7N-h5Hlpi__fjT_t0h","timestamp":1686678506179}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}