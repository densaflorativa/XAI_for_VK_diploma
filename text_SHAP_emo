{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ooPU4-ESi11i5dxYrwZ4AfNzaHN2Tosr","timestamp":1686688828056},{"file_id":"1iUt_-Is0J4hyF_wduHz0izk4yqxBWKGz","timestamp":1685980768367},{"file_id":"1A2Xah3HfS1OmjWTsWQ23EtIBFwTa6-C3","timestamp":1685493854527}],"gpuType":"T4","collapsed_sections":["ArdGxyiXgJWI","uvFgcr-84nOL","MyTxxe0Tgn-3"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2PyQFcITAq9B"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","source":["from typing import Dict, List\n","from transformers import AutoTokenizer, AutoModel\n","import numpy as np\n","import pandas as pd\n","import torch\n","from sklearn.metrics import f1_score, accuracy_score, classification_report\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from tqdm.auto import tqdm\n","import torch.nn.functional as F\n","import sklearn\n","import sys\n","import random\n","import math\n","import os\n","import argparse\n","import logging\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from torch.utils.data import Dataset, DataLoader\n"],"metadata":{"id":"_NGELs8mSTia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Dataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, df):\n","\n","        self.labels = [label for label in df['emo_class']]\n","        # self.labels = [labels[label] for label in df['round_N']]\n","        self.texts = [tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for text in df['text']]\n","\n","    def classes(self):\n","        return self.labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def get_batch_labels(self, idx):\n","        return np.array(self.labels[idx])\n","\n","    def get_batch_texts(self, idx):\n","        return self.texts[idx]\n","\n","    def __getitem__(self, idx):\n","\n","        batch_texts = self.get_batch_texts(idx)\n","        batch_y = self.get_batch_labels(idx)\n","\n","        return batch_texts, batch_y\n"],"metadata":{"id":"s-1ECZu1Suts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import AutoModel\n","\n","class BertBaseline(nn.Module):\n","    def __init__(self, model_name, inner_features, out_features):\n","        super(BertBaseline, self).__init__()\n","\n","        self.bert = AutoModel.from_pretrained(model_name, return_dict=True)\n","        self.conv = nn.Conv1d(768, 512, kernel_size=1)  # Convolutional layer for dimension reduction\n","        self.rnn = nn.GRU(512, inner_features, batch_first=True)\n","        self.linear1 = nn.Linear(inner_features, inner_features // 2)\n","        self.linear2 = nn.Linear(inner_features // 2, out_features)\n","        self.dropout = nn.Dropout(0.15)\n","        self.layer_norm = nn.LayerNorm(inner_features)\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        bert = self.bert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n","        hidden_states = bert.hidden_states\n","        x = hidden_states[-1]  # Use the last hidden layer\n","        x = x.transpose(1, 2)  # Transpose to make the dimension 768 the last dimension\n","        x = self.conv(x)  # Apply the convolutional layer for dimension reduction\n","        x = x.transpose(1, 2)  # Transpose back to the original dimension\n","        x = self.rnn(x)[0]  # Apply the RNN and get the output sequence\n","        x = self.layer_norm(x)  # Apply layer normalization\n","        x = self.dropout(x)  # Apply dropout\n","        x = self.relu(x)  # Apply ReLU activation\n","        x = x[:, -1, :]  # Take the last output from the recurrent layer\n","        x = self.linear1(x)  # Apply the first linear layer\n","        x = self.relu(x)  # Apply ReLU activation\n","        x = self.dropout(x)  # Apply dropout\n","        x = self.linear2(x)  # Apply the second linear layer\n","        x = self.softmax(x)  # Apply softmax activation\n","\n","        return x"],"metadata":{"id":"q89byYduS8IN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path_to_data = 'correct_tiny_features.csv'\n","target_value = 'emo_class'\n","path_to_model = 'xlm-roberta-base'\n","epochs = 10\n","seed_value = 42\n","\n","_ = seed_everything(seed_value)\n","LR = 1e-6\n","maxlength = 512\n","bsize = 15\n","\n","dataset = pd.read_csv(path_to_data)\n","\n","tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n","\n","from sklearn.model_selection import train_test_split\n","\n","df_train, df_test = train_test_split(dataset, test_size=0.3, stratify=dataset['emo_class'],\n","                                   random_state=42)\n","df_val, df_test = train_test_split(df_test, test_size=0.5, stratify=df_test['emo_class'],\n","                                   random_state=42)\n","low_class = df_train[df_train.psych_class == 2]\n","df_train = df_train.append(low_class)\n","df_train = df_train.reset_index(drop=True)\n","df_val = df_val.reset_index(drop=True)\n","\n","model = BertBaseline(path_to_model, inner_features=512, out_features=3)\n","\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"metadata":{"id":"a7dvDruGAzST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load('tiny_best_cnn_rnn_extra_emo7.pth'))"],"metadata":{"id":"bv42OsOPBgTc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model=model.to(device)"],"metadata":{"id":"Tkwxjtrrp9qZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_predictions(df):\n","\n","  new_list = []\n","\n","  for ind, row in tqdm(df.iterrows()):\n","\n","    label = row['emo_class']\n","\n","    tokenized = tokenizer(row['text'], padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n","    input_ids = tokenized[\"input_ids\"].to(device)\n","    attention_mask = tokenized[\"attention_mask\"].to(device)\n","\n","    with torch.no_grad():\n","        pred = model(input_ids, attention_mask)\n","\n","    predict = torch.log_softmax(pred, dim=1).argmax(dim=1)\n","    new_list.append(predict.cpu().detach().numpy()[0])\n","\n","  return new_list"],"metadata":{"id":"bEd6IkcgcLnm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds = get_predictions(dataset)\n","dataset[f'pred_emo_class'] = preds"],"metadata":{"id":"Dk0-ULwwcY9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[(dataset.sent_words_num >= 5) &(dataset.sent_words_num <= 100)]"],"metadata":{"id":"JtDnDCoZsmjk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_relevant_examples(df, n):\n","\n","  # def get_relevant(subsset):\n","  df = df.drop_duplicates(subset = 'text')\n","  df = df.sample(frac=1)\n","  max_class = df[(df['emo_class'] == 2) & (df[f'pred_emo_class'] == 2)]\n","  max_users = max_class.user_id.nunique()\n","  middle_class = df[(df['emo_class'] == 1) & (df[f'pred_emo_class'] == 1)]\n","  middle_users = middle_class.user_id.nunique()\n","  min_class = df[(df['emo_class'] == 0) & (df[f'pred_emo_class'] == 0)]\n","  min_users = min_class.user_id.nunique()\n","\n","  print(max_class.shape, middle_class.shape, min_class.shape)\n","  print(max_users, middle_users, min_users)\n","\n","  text_nums = []\n","  for i,pers_num in enumerate([max_users, middle_users, min_users]):\n","    if pers_num <= n:\n","      example_num = n//pers_num + 1\n","      text_nums.append(example_num)\n","    else:\n","      text_nums.append(1)\n","\n","\n","\n","  relevant_max = max_class.groupby('user_id').head(text_nums[0])\n","  relevant_middle = middle_class.groupby('user_id').head(text_nums[1])\n","  relevant_min = min_class.groupby('user_id').head(text_nums[2])\n","\n","\n","\n","  new_df = pd.concat([relevant_max.reset_index(drop=True)[:n], relevant_middle.reset_index(drop=True)[:n], relevant_min.reset_index(drop=True)[:n]])\n","  return new_df.reset_index(drop=True) relevant_max"],"metadata":{"id":"be8cy8NV5NeS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["res_df = get_relevant_examples(dataset, 50)"],"metadata":{"id":"bMIZsEx_c1lM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --quiet shap"],"metadata":{"id":"m4y2MFbLDQup"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["res_df"],"metadata":{"id":"njorFbjHaZH8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shap\n","print(f\"Shap version used: {shap.__version__}\")"],"metadata":{"id":"819qgjpEDwAL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize JS visualization for the notebook\n","shap.initjs()"],"metadata":{"id":"3oBK565mDyKq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install spicy"],"metadata":{"id":"1xXQnCiQD0A_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import spicy as sp"],"metadata":{"id":"4NWucX46D10h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def f(x):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=120, truncation=True) for v in x], device=device)\n","    attention_mask = (tv != 0).type(torch.int64)\n","    outputs = model(tv, attention_mask).detach().cpu()\n","    outputs = torch.log_softmax(outputs, dim=1).numpy()\n","    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n","    val = sp.special.logit(scores)\n","    return val"],"metadata":{"id":"j_S91N3kD32P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["explainer = shap.Explainer(f, tokenizer, output_names=[0,1,2])"],"metadata":{"id":"vkkBkW3pD5-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_texts = [res_df['text'].tolist()]"],"metadata":{"id":"H-2iajTCD8li"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shap_values = explainer(test_texts[0])"],"metadata":{"id":"sGNnrvGiEMGA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Высокий\n"],"metadata":{"id":"ArdGxyiXgJWI"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt"],"metadata":{"id":"G0VxB8_J6paR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shap.plots.text(shap_values[:10])"],"metadata":{"id":"7ykw2AAzjAWy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Средний"],"metadata":{"id":"MyTxxe0Tgn-3"}},{"cell_type":"code","source":["shap.plots.text(shap_values[50:60])"],"metadata":{"id":"Hh0Oi9Mt-kDK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Низкий"],"metadata":{"id":"dz6RwIoHevVB"}},{"cell_type":"code","source":["shap.plots.text(shap_values[-10:])"],"metadata":{"id":"tNZmNxU-e0nn"},"execution_count":null,"outputs":[]}]}